import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sklearn.linear_model
import os

os.chdir('C:\\Users\\User\\Desktop\\pyscripts')

########## Define Helper Functions ###########

def getPred(x,W):
    return(np.matmul(x,W))

def Loss(y,ypred):
    l=(y-ypred)**2
    return(l.sum())
    
def Cost(X,Y,W):
    cost=0
    m=X.shape[0]
    getPred=np.matmul(x,W)
    cost=sum((Y-getPred)**2)
    return((1/2*m)*cost)    
    

def GradDesc(X,Y,learnRate=0.01,epochs=2000,reg=0):
    
    global cacheLoss
    cacheLoss=[None]*epochs
    
    Weights=np.random.rand(X.shape[1])
    
    Weights=np.array(Weights)
    Weights=Weights.reshape(-1,1)
    m=X.shape[0]
    
    for i in range(epochs):
        
        predictions=getPred(X,Weights)
        cacheLoss[i]=Loss(Y,predictions)
        
        Weights[0]=Weights[0]-(1/m)*learnRate*(np.matmul(X[:,0].transpose(),predictions-Y))
        
        for j in range(1,len(Weights)):
            
            Weights[j]=Weights[j]-(1/m)*learnRate*(np.matmul(X[:,j].transpose(),predictions-Y)+sum(np.dot(Weights[j],reg)))


    plt.plot([i for i in range(epochs)],cacheLoss)
    plt.xlabel("Epoch")
    plt.ylabel("Cost")
    plt.title("Cost function per each epoch")
    plt.show()

    return(Weights)



def MSE(X,Y,W):
    return((1/X.shape[0])*sum((Y-np.matmul(X,W))**2))
    
    
    
    
#######################################################



### Read in Data and make Train-Dev and Test sets ###

cancerData=pd.read_csv('prostate.txt',delimiter='\t')

trainCancer=cancerData[cancerData.loc[:,'train']=='T']

testCancer=cancerData[cancerData.loc[:,'train']=='F']

x_train=trainCancer.drop(columns=['id','lpsa','train'])
y_train=trainCancer.loc[:,'lpsa']

x_test= testCancer.drop(columns=['id','lpsa','train'])
y_test=testCancer.loc[:,'lpsa']


x_train_scaled=sklearn.preprocessing.scale(x_train, axis=0, with_mean=True, with_std=True, copy=True)

x_test_scaled=sklearn.preprocessing.scale(x_test, axis=0, with_mean=True, with_std=True, copy=True)


# Linear Regression 

# Using scikit learn #
model=sklearn.linear_model.LinearRegression()
model.fit(x_train_scaled,y_train)

# Form into arrays with appropriate shape to plug into custom gradient descent, we had a column of ones for the intercept #

x_train_scaled=np.array(x_train_scaled)
y_train=np.array(y_train)
y_train=y_train.reshape(-1,1)

y_test=np.array(y_test)
y_test=y_test.reshape(-1,1)

addBias=np.ones([x_train_scaled.shape[0],1])

x_train_scaled=np.append(addBias,x_train_scaled,axis=1)

addBias=np.ones([x_test_scaled.shape[0],1])
x_test_scaled=np.append(addBias,x_test_scaled,axis=1)

GradDesc(x_train_scaled,y_train)
Wlinear=GradDesc(x_train_scaled,y_train)

# MSE on test set for linear regression
print(MSE(x_test_scaled,y_test,Wlinear))

# Using closed form #

w=np.matmul(np.matmul(np.linalg.inv(np.matmul(x_train_scaled.transpose(),x_train_scaled)),x_train_scaled.transpose()),y_train)

# Ridge Regression

# Form Train- Validation - Test set for tuning hyperparamater lambda

X_train, X_Validate, Y_train, Y_Validate = sklearn.model_selection.train_test_split( x_train_scaled, y_train, test_size=0.33, random_state=42)

def getRidge(x,y,reg=0):
    
    wRidge=np.matmul(np.matmul(np.linalg.inv(np.matmul(x.transpose(),x)+reg*np.identity(x.shape[1])),x.transpose()),y)
    
    return(wRidge)




def ChooseLambda(x,y):
    bestMSE=10e100
    
    lamList=[l*0.1 for l in range(0,100)]

    global bestWeight
    
    for l in lamList:
        W=getRidge(x,y,reg=l)
        if MSE(X_Validate,Y_Validate,W)< bestMSE:
            bestMSE=MSE(X_Validate,Y_Validate,W)
            bestLambda=l
            bestWeight=W
    return([bestMSE,bestLambda,bestWeight])



ChooseLambda(X_train,Y_train)

# The MSE on test set using Ridge Regression
print(MSE(x_test_scaled,y_test,bestWeight))




